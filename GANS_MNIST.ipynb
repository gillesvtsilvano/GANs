{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GANS - MNIST.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gillesvtsilvano/GANs/blob/master/GANS_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "-Phqh61W-ii5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Esse exemplo segue o tutorial que pode ser encontrado [aqui](https://hub.packtpub.com/generative-adversarial-networks-using-keras/).\n",
        "\n",
        "Para trabalhos futuros, observar:\n",
        "* [Step-by-step](https://www.kdnuggets.com/2017/10/seven-steps-deep-learning-keras.html).\n",
        "* Repositório do [Keras implementations of Generative Adversarial Networks](https://github.com/eriklindernoren/Keras-GAN)"
      ]
    },
    {
      "metadata": {
        "id": "Z6bnXGXs-uHh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introdução\n",
        "\n",
        "O dataset utilizado será o MNIST (Modified National Institute of Standards and Technology database), dataset clássico contendo 60,000 caracteres números (0-9) escritos à mão. Esse dataset não oferece nenhum desafio (já foram superados), porém, é um ótimo exemplo para prova de conceito ou para aprender alguma conteúdo relacionado ao aprendizado de máquina, nesse caso,  Generative Adversarial Networks (GANs)."
      ]
    },
    {
      "metadata": {
        "id": "aZbd2xL4EdPP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "O framework [Keras](https://keras.io/) já se encarregou de organizar o dataset, dividindo-o em duas partes: uma para treinamento, contendo 50,000 imagens, e outra para testes, contendo 10,000 imagens. "
      ]
    },
    {
      "metadata": {
        "id": "UfalsIZE_iaB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6a08ab66-d4d8-4b02-f33f-2e2c4fa5efd3"
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist #Loading MNIST dataset"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Lt0SEhZH_ZIV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<span style=\"color: red\">TODO</span>\n",
        "As GANs são compostas por duas redes neurais, uma Geradora (Generative) e outra Descritiva, que jogam _minmax_ uma contra a outra (Adversarial), no intuito de que a Descritiva evolua com base nos exemplos gerados pela Geradora, que aprenderá cada vez mais a gerar melhores exemplos, melhorando a qualidade do aprendizado da Descritiva.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ePz_N8N5DA74",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "  (X_train, _), (_, _) = mnist.load_data()\n",
        "  \"\"\"Um pixel vai de 0~255, mas a função de ativação do nosso exemplo\n",
        "  irá de -1 até 1¹\"\"\"\n",
        "  X_train = (X_tran.astype(np.float32) - 127.5) / 127.5\n",
        "  \"\"\"Não entendi. Alguma coisa com não utilzar os labels.\n",
        "  Resposta no Livro Deep Learning Quick Reference²\"\"\"\n",
        "  X_train = np.expand_dims(X_train, axis=3)\n",
        "  return X_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ae7c2sLWFwKA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "¹[Artigo sobre Standardization](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html)<br>\n",
        "²[Deep Learning Quick Reference](https://www.amazon.com/Deep-Learning-Quick-Reference-optimizing-ebook/dp/B0791JRGPY)"
      ]
    },
    {
      "metadata": {
        "id": "gpivQ7OQF74P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Constuindo a Rede Geradora\n",
        "\n",
        "Primeiro o código:"
      ]
    },
    {
      "metadata": {
        "id": "4cgIZC4HHtw_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_generator(noise_shape=(100, )):\n",
        "  inpt = Input(noise_shape)\n",
        "  x = Dense(128 * 7 * 7, activation=\"relu\")(inpt)\n",
        "  x = Reshape((7, 7, 128))(x)\n",
        "  x = BatchNormalization(momentum=0.8)(x)\n",
        "  x = UpSampling2D()(x)\n",
        "  x = Conv2D(128, kernel_size=3, padding=\"same\")(x)\n",
        "  x = Activation(\"relu\")(x)\n",
        "  x = BatchNormalization(momentum=0.8)(x)\n",
        "  x = UpSampling2D()(x)\n",
        "  x = Conv2D(64, kernel_size=3, padding=\"same\")(x)\n",
        "  x = Activation(\"relu\")(x)\n",
        "  x = BatchNormalization(momentum=0.8)(x)\n",
        "  x = Conv2D(1, kernel_size=3, padding=\"same\")(x)\n",
        "  out = Activation(\"tanh\")(x)\n",
        "  model = Model(inpt, out)\n",
        "  print(\"--- Generator ---\")\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EBJz61G2I7tE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We have not previously used the UpSampling2D layer. This layer will take increases in the rows and columns of the input tensor, leaving the channels unchanged. It does this by repeating the values in the input tensor. By default, it will double the input. If we give an UpSampling2D layer a 7 x 7 x 128 input, it will give us a 14 x 14 x 128 output.\n",
        "\n",
        "Typically when we build a CNN, we start with an image that is very tall and wide and uses convolutional layers to get a tensor that’s very deep but less tall and wide. Here we will do the opposite. We’ll use a dense layer and a reshape to start with a 7 x 7 x 128 tensor and then, after doubling it twice, we’ll be left with a 28 x 28 tensor. Since we need a grayscale image, we can use a convolutional layer with a single unit to get a 28 x 28 x 1 output.\n",
        "\n",
        "This sort of generator arithmetic is a little off-putting and can seem awkward at first but after a few painful hours, you will get the hang of it!"
      ]
    },
    {
      "metadata": {
        "id": "s-THs45KKOlt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Construindo a Descriminante\n",
        "\n",
        "A Descriminante é, em grande parte, a mesma que qualquer outra CNN. "
      ]
    },
    {
      "metadata": {
        "id": "7jNgere9KgaG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_descriminator(img_shape):\n",
        "  inpt = Input(img_shape)\n",
        "  x = Conv2D(32, kernel_size=3, strides=2, padding=\"same\")(inpt)\n",
        "  x = LeakyReLU(alpha=0.2)(x)\n",
        "  x = Dropout(0.25)(x)\n",
        "  x = Conv2D(64, kernel_size=3, strides=2, padding=\"same\")(x)\n",
        "  x = ZeroPadding2D(padding=((0, 1), (0, 1)))(x)\n",
        "  x = (LeakyReLU(alpha=0.2))(x)\n",
        "  x = Dropout(0.25)(x)\n",
        "  x = BatchNormalization(momentum=0.8)(x)\n",
        "  x = Conv2D(128, kernel_size=3, strides=2, padding=\"same\")(x)\n",
        "  x = LeakyReLU(alpha=0.2)(x)\n",
        "  x = BatchNormalization(momentum=0.8)(x)\n",
        "  x = Conv2D(256, kernel_size=3, strides=1, padding=\"same\")(x)\n",
        "  x = LeakyReLU(alpha=0.2)(x)\n",
        "  x = Dropout(0.25)(x)\n",
        "  x = Flatten()(x)\n",
        "  out = Dense(1, activation='sigmoid')(x)\n",
        "  model = Model(inpt, out)\n",
        "  print(\"--- DIscriminator ---\")\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4sKCrUmrLwcj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}